# To be filled by the author(s) at the time of submission
# -------------------------------------------------------

# Title of the article:
#  - For a successful replication, it should be prefixed with "[Re]"
#  - For a failed replication, it should be prefixed with "[Â¬Re]"
#  - For other article types, no instruction (but please, not too long)
title: "[Re] Teaching CLIP to Count to Ten"

# List of authors with name, orcid number, email and affiliation
# Affiliation "*" means contact author (required even for single-authored papers)
# To include author suffix format name as Last Suffix, First M.I. (e.g. Schackart III, Kenneth E.)
authors:
  - name: Harshvardhan Mestha
    email: harshvardhanmestha@yahoo.com
    affiliations: 1,2,*

  - name: Tejas Agrawal
    email: tejasagrawal55@gmail.com
    affiliations: 1,2

  - name: Karan Bania
    email: karan.bania.9@gmail.com
    affiliations: 1,2

  - name: Shreyas V
    email: shreyas.college@gmail.com
    affiliations: 1,2

  - name: Yash Bhisikar
    email: yashbhisikar24@gmail.com
    affiliations: 1,2
    


# List of affiliations with code (corresponding to author affiliations), name
# and address. You can also use these affiliations to add text such as "Equal
# contributions" as name (with no address).
affiliations:
  - code:    1
    name:    SAiDL
    address: Goa, India
    
  - code:    2
    name:    BITS Pilani
    address: Goa, India


# List of keywords (adding the programming language might be a good idea)
keywords: rescience c, rescience x, pytorch, VLMs, CLIP, Computer Vision

# Code URL and DOI/SWH (url is mandatory for replication, doi after acceptance)
# You can get a DOI for your code from Zenodo, or an SWH identifier from
# Software Heritage.
#   see https://guides.github.com/activities/citable-code/
code:
  - url: https://github.com/SforAiDl/CountCLIP
  - doi: 
  - swh: 

# Data URL and DOI (optional if no data)
data:
  - url: https://doi.org/10.5281/zenodo.10981852
  - doi: 10.5281/zenodo.10981852

# Information about the original article that has been replicated
replication:
 - cite: Teaching CLIP to Count to Ten
 - bib:  paiss2023teaching
 - url:  https://arxiv.org/pdf/2302.12066
 - doi:  https://doi.org/10.48550/arXiv.2302.12066

# Don't forget to surround abstract with double quotes
abstract: "Large vision-language models (VLMs), such as CLIP, learn rich joint image-text representations, facilitating advances in numerous downstream tasks, including zero-shot classification and text-to-image generation. Nevertheless, existing VLMs exhibit a prominent well-documented limitation - they fail to encapsulate compositional concepts such as counting. We introduce a simple yet effective method to improve the quantitative understanding of VLMs, while maintaining their overall performance on common benchmarks. Specifically, we propose a new counting-contrastive loss used to finetune a pre-trained VLM in tandem with its original objective. Our counting loss is deployed over automatically-created counterfactual examples, each consisting of an image and a caption containing an incorrect object count. For example, an image depicting three dogs is paired with the caption \"Six dogs playing in the yard\". Our loss encourages discrimination between the correct caption and its counterfactual variant which serves as a hard negative example. To the best of our knowledge, this work is the first to extend CLIP's capabilities to object counting. Furthermore, we introduce \"CountBench\" - a new image-text counting benchmark for evaluating a model's understanding of object counting. We demonstrate a significant improvement over state-of-the-art baseline models on this task. Finally, we leverage our count-aware CLIP model for image retrieval and text-conditioned image generation, demonstrating that our model can produce specific counts of objects more reliably than existing ones."

# Bibliography file (yours)
bibliography: bibliography.bib
  
# Type of the article
# Type can be:
#  * Editorial
#  * Letter
#  * Replication
type: Replication

# Scientific domain of the article (e.g. Computational Neuroscience)
#  (one domain only & try to be not overly specific)
domain: Computer Vision

# Coding language (main one only if several)
language: Python

  
# To be filled by the author(s) after acceptance
# -----------------------------------------------------------------------------

# For example, the URL of the GitHub issue where review actually occured
review: 
  - url: 

contributors:
  - name:
    orcid: 
    role: editor
  - name:
    orcid:
    role: reviewer
  - name:
    orcid:
    role: reviewer

# This information will be provided by the editor
dates:
  - received:  November 1, 2018
  - accepted:
  - published: 

# This information will be provided by the editor
article:
  - number: # Article number will be automatically assigned during publication
  - doi:    # DOI from Zenodo
  - url:    # Final PDF URL (Zenodo or rescience website?)

# This information will be provided by the editor
journal:
  - name:   "ReScience C"
  - issn:   2430-3658
  - volume: 4
  - issue:  1
